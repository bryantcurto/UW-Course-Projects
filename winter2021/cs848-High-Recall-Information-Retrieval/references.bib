@inproceedings{10.1145/564376.564429,
author = {Cronen-Townsend, Steve and Zhou, Yun and Croft, W. Bruce},
title = {Predicting Query Performance},
year = {2002},
isbn = {1581135610},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/564376.564429},
doi = {10.1145/564376.564429},
abstract = {We develop a method for predicting query performance by computing the relative entropy between a query language model and the corresponding collection language model. The resulting clarity score measures the coherence of the language usage in documents whose models are likely to generate the query. We suggest that clarity scores measure the ambiguity of a query with respect to a collection of documents and show that they correlate positively with average precision in a variety of TREC test sets. Thus, the clarity score may be used to identify ineffective queries, on average, without relevance information. We develop an algorithm for automatically setting the clarity score threshold between predicted poorly-performing queries and acceptable queries and validate it using TREC data. In particular, we compare the automatic thresholds to optimum thresholds and also check how frequently results as good are achieved in sampling experiments that randomly assign queries to the two classes.},
booktitle = {Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {299–306},
numpages = {8},
keywords = {ambiguity, clarity, information theory, language models},
location = {Tampere, Finland},
series = {SIGIR '02}
}
@inproceedings{10.1145/290941.291008,
author = {Ponte, Jay M. and Croft, W. Bruce},
title = {A Language Modeling Approach to Information Retrieval},
year = {1998},
isbn = {1581130155},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/290941.291008},
doi = {10.1145/290941.291008},
booktitle = {Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {275–281},
numpages = {7},
location = {Melbourne, Australia},
series = {SIGIR '98}
}
@inproceedings{10.1145/502585.502654,
author = {Zhai, Chengxiang and Lafferty, John},
title = {Model-Based Feedback in the Language Modeling Approach to Information Retrieval},
year = {2001},
isbn = {1581134363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502585.502654},
doi = {10.1145/502585.502654},
abstract = {The language modeling approach to retrieval has been shown to perform well empirically. One advantage of this new approach is its statistical foundations. However, feedback, as one important component in a retrieval system, has only been dealt with heuristically in this new retrieval approach: the original query is usually literally expanded by adding additional terms to it. Such expansion-based feedback creates an inconsistent interpretation of the original and the expanded query. In this paper, we present a more principled approach to feedback in the language modeling approach. Specifically, we treat feedback as updating the query language model based on the extra evidence carried by the feedback documents. Such a model-based feedback strategy easily fits into an extension of the language modeling approach. We propose and evaluate two different approaches to updating a query language model based on feedback documents, one based on a generative probabilistic model of feedback documents and one based on minimization of the KL-divergence over feedback documents. Experiment results show that both approaches are effective and outperform the Rocchio feedback approach.},
booktitle = {Proceedings of the Tenth International Conference on Information and Knowledge Management},
pages = {403–410},
numpages = {8},
location = {Atlanta, Georgia, USA},
series = {CIKM '01}
}
@inproceedings{10.1145/383952.383972,
author = {Lavrenko, Victor and Croft, W. Bruce},
title = {Relevance Based Language Models},
year = {2001},
isbn = {1581133316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/383952.383972},
doi = {10.1145/383952.383972},
abstract = {We explore the relation between classical probabilistic models of information retrieval and the emerging language modeling approaches. It has long been recognized that the primary obstacle to effective performance of classical models is the need to estimate arelevance model: probabilities of words in the relevant class. We propose a novel technique for estimating these probabilities using the query alone. We demonstrate that our technique can produce highly accurate relevance models, addressing important notions of synonymy and polysemy. Our experiments show relevance models outperforming baseline language modeling systems on TREC retrieval and TDT tracking tasks. The main contribution of this work is an effective formal method for estimating a relevance model with no training data.},
booktitle = {Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {120–127},
numpages = {8},
location = {New Orleans, Louisiana, USA},
series = {SIGIR '01}
}
@misc{buettcher2009wumpus,
  title={The Wumpus Information Retrieval System—On-disk index data structures},
  author={Buettcher, Stefan},
  year={2009}
}
@article{10.1007/s10791-006-9006-4,
author = {Cronen-Townsend, Steve and Zhou, Yun and Croft, W. Bruce},
title = {Precision Prediction Based on Ranked List Coherence},
year = {2006},
issue_date = {December  2006},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {9},
number = {6},
issn = {1386-4564},
url = {https://doi.org/10.1007/s10791-006-9006-4},
doi = {10.1007/s10791-006-9006-4},
abstract = {We introduce a statistical measure of the coherence of a list of documents called the  clarity score . Starting with a document list ranked by the query-likelihood retrieval model, we demonstrate the score's relationship to query ambiguity with respect to the collection. We also show that the clarity score is correlated with the average precision of a query and lay the groundwork for useful predictions by discussing a method of setting decision thresholds automatically. We then show that passage-based clarity scores correlate with average-precision measures of ranked lists of passages, where a passage is judged relevant if it contains correct answer text, which extends the basic method to passage-based systems. Next, we introduce variants of document-based clarity scores to improve the robustness, applicability, and predictive ability of clarity scores. In particular, we introduce the ranked list clarity score that can be computed with only a ranked list of documents, and the weighted clarity score where query terms contribute more than other terms. Finally, we show an approach to predicting queries that perform poorly on query expansion that uses techniques expanding on the ideas presented earlier.},
journal = {Inf. Retr.},
month = dec,
pages = {723–755},
numpages = {33},
keywords = {Language models, Query expansion, Performance prediction}
}
@inproceedings{10.1145/319950.320022,
author = {Song, Fei and Croft, W. Bruce},
title = {A General Language Model for Information Retrieval},
year = {1999},
isbn = {1581131461},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/319950.320022},
doi = {10.1145/319950.320022},
abstract = {Statistical language modeling has been successfully used for speech recognition, part-of-speech tagging, and syntactic parsing. Recently, it has also been applied to information retrieval. According to this new paradigm, each document is viewed as a language sample, and a query as a generation process. The retrieved documents are ranked based on the probabilities of producing a query from the corresponding language models of these documents. In this paper, we will present a new language model for information retrieval, which is based on a range of data smoothing techniques, including the Good-Turning estimate, curve-fitting functions, and model combinations. Our model is conceptually simple and intuitive, and can be easily extended to incorporate probabilities of phrases such as word pairs and word triples. The experiments with the Wall Street Journal and TREC4 data sets showed that the performance of our model is comparable to that of INQUERY and better than that of another language model for information retrieval. In particular, word pairs are shown to be useful in improving the retrieval performance.},
booktitle = {Proceedings of the Eighth International Conference on Information and Knowledge Management},
pages = {316–321},
numpages = {6},
keywords = {statistical language modeling, model combinations, curve-fitting functions, good-turing estimate},
location = {Kansas City, Missouri, USA},
series = {CIKM '99}
}
@article{10.1145/984321.984322,
author = {Zhai, Chengxiang and Lafferty, John},
title = {A Study of Smoothing Methods for Language Models Applied to Information Retrieval},
year = {2004},
issue_date = {April 2004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/984321.984322},
doi = {10.1145/984321.984322},
abstract = {Language modeling approaches to information retrieval are attractive and promising because they connect the problem of retrieval with that of language model estimation, which has been studied extensively in other application areas such as speech recognition. The basic idea of these approaches is to estimate a language model for each document, and to then rank documents by the likelihood of the query according to the estimated language model. A central issue in language model estimation is smoothing, the problem of adjusting the maximum likelihood estimator to compensate for data sparseness. In this article, we study the problem of language model smoothing and its influence on retrieval performance. We examine the sensitivity of retrieval performance to the smoothing parameters and compare several popular smoothing methods on different test collections. Experimental results show that not only is the retrieval performance generally sensitive to the smoothing parameters, but also the sensitivity pattern is affected by the query type, with performance being more sensitive to smoothing for verbose queries than for keyword queries. Verbose queries also generally require more aggressive smoothing to achieve optimal performance. This suggests that smoothing plays two different role---to make the estimated document language model more accurate and to "explain" the noninformative words in the query. In order to decouple these two distinct roles of smoothing, we propose a two-stage smoothing strategy, which yields better sensitivity patterns and facilitates the setting of smoothing parameters automatically. We further propose methods for estimating the smoothing parameters automatically. Evaluation on five different databases and four types of queries indicates that the two-stage smoothing method with the proposed parameter estimation methods consistently gives retrieval performance that is close to---or better than---the best results achieved using a single smoothing method and exhaustive parameter search on the test data.},
journal = {ACM Trans. Inf. Syst.},
month = apr,
pages = {179–214},
numpages = {36},
keywords = {backoff smoothing, absolute discounting smoothing, term weighting, interpolation smoothing, EM algorithm, Statistical language models, risk minimization, TF-IDF weighting, Jelinek--Mercer smoothing, Dirichlet prior smoothing, leave-one-out, two-stage smoothing}
}
@inproceedings{collins2009reducing,
  title={Reducing the risk of query expansion via robust constrained optimization},
  author={Collins-Thompson, Kevyn},
  booktitle={Proceedings of the 18th ACM conference on Information and knowledge management},
  pages={837--846},
  year={2009}
}
@inproceedings{harman2004nrrc,
  title={The NRRC reliable information access (RIA) workshop},
  author={Harman, Donna and Buckley, Chris},
  booktitle={Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval},
  pages={528--529},
  year={2004}
}
@inproceedings{10.1145/1835449.1835546,
author = {Lv, Yuanhua and Zhai, ChengXiang},
title = {Positional Relevance Model for Pseudo-Relevance Feedback},
year = {2010},
isbn = {9781450301534},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1835449.1835546},
doi = {10.1145/1835449.1835546},
abstract = {Pseudo-relevance feedback is an effective technique for improving retrieval results. Traditional feedback algorithms use a whole feedback document as a unit to extract words for query expansion, which is not optimal as a document may cover several different topics and thus contain much irrelevant information. In this paper, we study how to effectively select from feedback documents those words that are focused on the query topic based on positions of terms in feedback documents. We propose a positional relevance model (PRM) to address this problem in a unified probabilistic way. The proposed PRM is an extension of the relevance model to exploit term positions and proximity so as to assign more weights to words closer to query words based on the intuition that words closer to query words are more likely to be related to the query topic. We develop two methods to estimate PRM based on different sampling processes. Experiment results on two large retrieval datasets show that the proposed PRM is effective and robust for pseudo-relevance feedback, significantly outperforming the relevance model in both document-based feedback and passage-based feedback.},
booktitle = {Proceedings of the 33rd International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {579–586},
numpages = {8},
keywords = {positional relevance model, query expansion, positional language model, passage-based feedback, pseudo relevance feedback, proximity},
location = {Geneva, Switzerland},
series = {SIGIR '10}
}